# 技术总结

## 线程同步
线程同步的四项原则，按重要性排列：
1. 首要原则是尽量最低限度地共享对象，减少需要同步的场合。一个对象能不暴露给别的线程就不要暴露；如果要暴露，优先考虑不可修改的对象；实在不行才暴露可修改的对象，并用同步措施来充分保护它。
2. 其次是使用高级的并发编程构件，如TaskQueue、Producer-Consumer Queue、 CountDownLatch等。
3. 最后不得已必须使用底层同步原语时，只用非递归的互斥器和条件变量，慎用读写锁，不要用信号量。
4. 除了使用atomic整数之外，不自己编写lock-free代码，也不要用“内核级”同步原语。

### 互斥锁
使用RAII（资源获取即初始化）技术，对MutexLock初始化和析构进行处理。MutexLockGuard对象初始化的时候加锁，析构的时候解锁。
```
class MutexLockGuard : noncopyable
{
public:
  explicit MutexLockGuard(MutexLock& mutex) : mutex_(mutex)
  {
    mutex_.lock();
  }

  ~MutexLockGuard()
  {
    mutex_.unlock();
  }

private:
  MutexLock& mutex_;
};
```
### 条件变量
1. Blockingqueue

关于封装条件变量的一般使用，假设我们要实现简单的容量无限的BlockingQueue，可以这样写：
```
MutexLock mutex;
Condition cond(mutex);
std::deque<int> queue;

int dequeue()
{
    MutexLockGurad lock(mutex);
    while(queue.empty())
    {
        cond.wait();//这一步会释放mutex并进入等待，这两个操作是原子的
        //wait()返回后，会自动重新加锁
    }
    assert(!queue.empty());
    int top = queue.front();
    queue.pop_front();
    return top;
}

void enqueue(int x )
{
    MutexLockGurad(mutex);
    queue.push_back(x);
    cond.notify();
}
```

2. CountDownLatch

CountDownLatch,本质上来说,是一个thread safe的计数器,用于主线程和工作线程的同步.
它的主要用法有两种:
* 第一种:在初始化时,需要指定主线程需要等待的任务的个数(count),当工作线程完成 Task Callback后对计数器减1，而主线程通过wait()调用阻塞等待技术器减到0为止。**通常用于主线程等待多个子线程完成初始化。**
* 第二种:主线程发起多个子线程，子线程都等待主线程，主线程完成其他任务之后通知所有子线程开始执行。**通常用于多个子线程等待主线程发出起跑命令。**

```
class CountDownLatch : boost::noncopyable
{
 public:

  explicit CountDownLatch(int count)
    : mutex_(),
      condition_(mutex_),
      count_(count)
  {
  }

  void wait()
  {
    MutexLockGuard lock(mutex_);
    while (count_ > 0)
    {
      condition_.wait();
    }
  }

  void countDown()
  {
    MutexLockGuard lock(mutex_);
    --count_;
    if (count_ == 0)
    {
      condition_.notifyAll();
    }
  }

  int getCount() const
  {
    MutexLockGuard lock(mutex_);
    return count_;
  }

 private:
  mutable MutexLock mutex_;
  Condition condition_;
  int count_;
};
```

## 多线程日志
一个多线程程序的每个线程只写一个日志文件，这样分析日志更容易，不必在多个文件中跳来跳去，多线程写多个文件也不一定能提速。
采用“异步日志”的方式，用一个背景线程负责收集日志消息，并写入日志文件, 其他业务线程只管往这个“日志线程”发送日志消息。

**日志库采用的是双缓冲技术，基本思路是准备两块buffer：A和B。前端负责往buffer A填数据，后端负责将buffer B的数据写人文件。
当buffer A写满之后，交换A和B，让后端将buffer A的数据写入文件，而前端则往buffer B填人新的日志消息，如此往复。**

用两个buffer的好处是：在新建日志消息的时候不必等待磁盘文件操作，也避免每条新日志消息都触发后端日志线程。
换言之，前端不是将一条条日志消息分别传送给后端，而是将多条日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频度，降低开销。
另外，为了及时将日志消息写人文件，即便buffer A未满，日志库也会每3秒执行一次上述交换写人操作。

**日志大体可以分为前端和后端两部分。前端是供应用程序使用的接口API，并生成日志消息；后端负责把日志消息写到目的地。**

关键代码 base/AsyncLogging.cpp
Buffer 大小为4MB，可以存至少1000条日志消息。mutex_用于保护后面的四个数据成员。buffers_放的是供后端写入的buffer。
```
MutexLock mutex_;
Condition cond_;
BufferPtr currentBuffer_;
BufferPtr nextBuffer_;
BufferVector buffers_;

void append(const char* logline, int len) {
    muduo::MutexLockGuard lock(mutex_);
    if (currentBuffer_->avail() > len)
    {
        currentBuffer_->append(logline, len);
    }
    else
    {
        buffers_.push_back(currentBuffer_.release());
        if (nextBuffer_)
        {
            currentBuffer_ = boost::ptr_container::move(nextBuffer_);
        }
        else
        {
            currentBuffer_.reset(new Buffer); // Rarely happens
        }
        currentBuffer_->append(logline, len);
        cond_.notify();
    }
}

void AsyncLogging::threadFunc() {//后端接收日志
  assert(running_ == true);
  latch_.countDown();
  LogFile output(basename_);
  BufferPtr newBuffer1(new Buffer);//准备两块空闲的日志
  BufferPtr newBuffer2(new Buffer);
  newBuffer1->bzero();
  newBuffer2->bzero();
  BufferVector buffersToWrite;
  buffersToWrite.reserve(16);
  while (running_) {
    assert(newBuffer1 && newBuffer1->length() == 0);
    assert(newBuffer2 && newBuffer2->length() == 0);
    assert(buffersToWrite.empty());

    {
      MutexLockGuard lock(mutex_);
      if (buffers_.empty())  // unusual usage!
      {
        cond_.waitForSeconds(flushInterval_);//触发条件：1，超时 2，前端写满了一个或者多个buffer
      }
      buffers_.push_back(currentBuffer_);
      currentBuffer_.reset();

      currentBuffer_ = std::move(newBuffer1);//将空闲的newBuffer1移为当前缓冲
      buffersToWrite.swap(buffers_);//内部指针交换而非复制，这样代码可以在临界区之外安全的访问buffersToWrite
      if (!nextBuffer_) {
        nextBuffer_ = std::move(newBuffer2);//neWBuffer2替换nextBuffer_，这样前端始终有一个预备buffer可供调配
      }
    }

    assert(!buffersToWrite.empty());

    if (buffersToWrite.size() > 25) {//处理过多的日志堆积问题，直接丢弃
      char buf[256];
      snprintf(buf, sizeof buf, "Dropped log messages at %s, %zd larger buffers\n",
                Timestamp::now().toFormattedString().c_str(),
                buffersToWrite.size()-2);
      fputs(buf, stderr);
      output.append(buf, static_cast<int>(strlen(buf)));
      buffersToWrite.erase(buffersToWrite.begin() + 2, buffersToWrite.end());
    }

    for (size_t i = 0; i < buffersToWrite.size(); ++i) {//将buffersToWrite写到文件中
      output.append(buffersToWrite[i]->data(), buffersToWrite[i]->length());
    }

    if (buffersToWrite.size() > 2) {
      // drop non-bzero-ed buffers, avoid trashing
      buffersToWrite.resize(2);
    }

    if (!newBuffer1) {
      assert(!buffersToWrite.empty());
      newBuffer1 = buffersToWrite.back();
      buffersToWrite.pop_back();
      newBuffer1->reset();
    }

    if (!newBuffer2) {
      assert(!buffersToWrite.empty());
      newBuffer2 = buffersToWrite.back();
      buffersToWrite.pop_back();
      newBuffer2->reset();
    }

    buffersToWrite.clear();
    output.flush();
  }
  output.flush();
}
```

前端在生成一条日志消息的时候会调用AsyncLogging::append();.
1. 如果当前缓冲currentBuffer_剩余的空间足够大，则会直接把日志消息追加到当前缓冲中，这是最常见的情况。这里拷贝一条日志消息并不会带来多大开销。前后端代码的其余部分都没有拷贝，而是简单的指针交换；
2. 如果当前缓冲已经写满，就把它移入buffers_，并试图把预备好的另一块缓冲nextBuffer_移用为当前缓冲；然后追加日志消息并唤醒后端开始写人日志数据。
以上两种情况在临界区之内都没有耗时的操作，运行时间为常数；
3. 如果前端写人速度太快，一下子把两块缓冲都用完了，那么只好分配一块新的buffer作为当前缓冲，这是极少发生的情况。

在接收方（后端）实现AsyncLogging::threadFunc中:
1. 首先准备好两块空闲的buffer，以备在临界区内交换；在临界区内，等待条件触发，这里的条件有两个：其一是超时，其二是前端写满了一个或多个buffer。注意这里是非常规的condition variable用法，它没有使用while循环，而且等待时间有上限。
2. 当“条件”满足时，先将当前缓冲currentBuffer_移人buffers_，并立刻将空闲的newBuffer1移为当前缓冲。这整段代码位于临界区之内，因此不会有任何race condition。
3. 接下来将交换buffers_和buffersToWrite，后面的代码可以在临界区之外安全地访问bufferSToWrite，将其中的数据写入文件。
4. 临界区里最后干的一件事情是用neWBuffer2替换nextBuffer_，这样前端始终有一个预备buffer可供调配。nextBuffer_可以减少前端临界区分配内存的概率，缩短前端临界区长度。注意到后端临界区内也没有耗时的操作，运行时间为常数。
5. 接下来会将 buffersToWrite 内的 buffer 重新填充 newBufferl 和 newBuffer2，这样下一次执行的时候还有两个空闲buffer可用于替换前端的当前缓冲和预备缓冲。

## 网络核心库
本项目是基于Reactor模式的网络库，其核心是个事件循环EventLoop，用于相应计时器和IO事件。
reactor最核心的事件分发机制，即将IO multiplexing 拿到的IO事件分发给各个文件描述符(fd)的事件处理函数。

主要部件：
1. Buffer：数据的读写通过buffer进行。用户代码不需要调用read()/write()，只需要处理收到的数据和准备好要发送的数据
2. Acceptor：监听listen socket，accept新的连接
3. Socket：封装Socket描述符
3. EventLoop：事件循环（反应器Reactor），每个线程只能有一个 EventLoop实体，它负责IO和定时器事件的分派。它用eventfd()来异步唤醒，这有别于传统的用一对pipe()的办法。
它用TimerQueue作为计时器管理，用Poller作为IO multiplexing的实现
4. EventLoopThread：启动一个线程，在其中运行EventLoop::loop()
5. Channel：用于socket连接事件的分发
6. TcpConnection：整个网络库的核心，封装一次TCP连接，注意它不能发起连接
7. TcpClient：用于编写网络客户端，能发起连接，并且有重试功能
8. TcpServer：用于编写网络服务器，接受客户的连接

## 并发模型

本网络库是并发非阻塞TCP网络编程，它的核心是每个IO线程是一个事件循环，把IO事件分发到回调函数上。
编写网络库的目的之一就是简化日常的TCP网络编程，让程序员能把精力集中在业务逻辑的实现上，而不要天天和Sockets API较劲。

基于事件的非阻塞网络编程是编写高性能并发网络服务程序的主流模式：
头一次使用这种方式编程通常需要转换思维模式：把原来“主动调用recv()来接收数据，主动调用accept()来接受新连接，主动调用send()来发送数据”的思路换成
“注册一个收数据的回调，网络库收到数据会调用我，直接把数据提供给我，供我消费。注册一个接受连接的回调，网络库接受了新连接会回调我，直接把新的连接对象传给我，供我使用。
需要发送数据的时候，只管往连接中写，网络库会负责无阻塞地发送。”

本项目使用的并发模型如下：

![并发模型](https://github.com/coffee0218/WebServer/blob/master/data/model.png)

并发模型的时序图如下：

![并发模型](https://github.com/coffee0218/WebServer/blob/master/data/model.png)

每个线程最多有一个EventLoop，每个TcpConnection必须归某个EventLoop管理。MainReactor只有一个，负责响应client的连接请求，并建立连接，它使用一个NIO Selector。
Acceptor与MainEventLoop在同一个线程，另外创建一个EventLoopThreadPool，新的连接会按round-robin方式分配到线程中，线程池的大小是固定的，池子的大小通常根据CPU数目确定。

新的TcpConnection连接挂在某个subReactor中，该连接的所有操作都在那个subReactor所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用CPU。
每个subReactor都会在一个独立线程中运行，属于某个EventLoop，并且维护一个独立的NIO Selector。

一个连接完全由MainReactor一个线程管理，请求的顺序性得到保证，同时IO的请求压力比较大时，subReactor也可以分担。
当主线程把新连接分配给了某个SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，怎么得知新连接的到来呢？这里使用了eventfd进行异步唤醒，线程会从epoll_wait中醒来，得到活跃事件，进行处理。

## Buffer设计
Non-blocking IO 的核心思想是避免阻塞在 read() 或 write() 或其他 IO 系统调用上，这样可以最大限度地复用 thread-of-control，让一个线程能服务于多个 socket 连接。
IO 线程只能阻塞在 IO-multiplexing 函数上，如 select()/poll()/epoll_wait()。这样一来，应用层的缓冲是必须的，每个 TCP socket 都要有input buffer 和 output buffer。
项目对I/O做了一层封装，让网络库与真正的套接字打交道，在应用层只需要利用封装的Buffer完成数据的收发。

Buffer 其实像是一个 queue，从末尾写入数据，从头部读出数据。
•	对外表现为一块连续的内存(char* p, int len)，以方便客户代码的编写。 
•	其 size() 可以自动增长，以适应不同大小的消息。它不是一个 fixed size array (即 char buf[8192])。 
•	内部以 vector of char 来保存数据，并提供相应的访问函数。 

Buffer 的数据结构如下：

![并发模型](https://github.com/coffee0218/WebServer/blob/master/data/buffer.png)

两个 indices 把 vector 的内容分为三块：prependable、readable、writable，各块的大小是（公式一）：
* prependable = readIndex
* readable = writeIndex - readIndex
* writable = size() - writeIndex

readIndex 和 writeIndex 满足以下不变式(invariant): 0 ≤ readIndex ≤ writeIndex ≤ data.size()

Buffer如何读取数据，一方面我们希望减少系统调用，一次读的数据越多越划算，那么似乎应该准备一个大的缓冲区。另一方面，我们系统减少内存占用。
如果有 10k 个连接，每个连接一建立就分配 64k 的读缓冲的话，将占用 640M 内存，而大多数时候这些缓冲区的使用率很低。用 readv 结合栈上空间巧妙地解决了这个问题。

Buffer的做法比较巧妙，Buffer底层默认会有一个1KB大小的vector，除此之外，在栈上申请一个65536字节大小的临时空间，当网络套接字数据太大时，可以将超出1K的数据先放入这个临时空间，
然后在append到Buffer里面（这个时候Buffer就会resize一个适应的大小）。

因为使用read()将数据读到不连续的内存、使用write()将不连续的内存发送出去，要经过多次的调用read、write。
UNIX提供了另外两个函数readv()和writev()，它们只需一次系统调用就可以实现在文件和进程的多个缓冲区之间传送数据，免除了多次系统调用或复制数据的开销。
在一次函数调用中：
* writev以顺序iov[0]、iov[1]至iov[iovcnt-1]从各缓冲区中聚集输出数据到fd
* readv则将从fd读入的数据按同样的顺序散布到各缓冲区中，readv总是先填满一个缓冲区，然后再填下一个

// 结合栈上的空间，避免内存使用过大，提高内存使用率
```
ssize_t Buffer::readFd(int fd, int *savedErrno)
{
    // 节省一次ioctl系统调用（获取有多少可读数据）
    char extrabuf[65536];
    struct iovec vec[2];
    const size_t writable = writableBytes();
    // 第一块缓冲区
    vec[0].iov_base = begin() + writerIndex_;
    vec[0].iov_len = writable;
    // 第二块缓冲区
    vec[1].iov_base = extrabuf;
    vec[1].iov_len = sizeof extrabuf;
    const ssize_t n = sockets::readv(fd, vec, 2);
    if (n < 0)
    {
        *savedErrno = errno;
    }
    else if (implicit_cast<size_t>(n) <= writable)  //第一块缓冲区足够容纳
    {
        writerIndex_ += n;
    }
    else        // 当前缓冲区，不够容纳，因而数据被接收到了第二块缓冲区extrabuf，将其append至buffer
    {
        writerIndex_ = buffer_.size();
        append(extrabuf, n - writable);
    }

    return n;
}
```
## TimerQueue定时器

传统的Reactor通过控制select和poll的等待事件来实现定时，而现在在linux中有了timerfd，我们可以用和处理IO事件相同的方式来处理定时，代码的一致性更好。
TimerQueue只实现addTimer(), cancel(). addTimer()是提EventLoop使用的，EventLoop会把它封装成为更好用的runAt()，runAfter()，runEvery()等函数。
```
TimerId EventLoop::runAt(const Timestamp& time, const TimerCallback& cb)
{
  return timerQueue_->addTimer(cb, time, 0.0);
}

TimerId EventLoop::runAfter(double delay, const TimerCallback& cb)
{
  Timestamp time(addTime(Timestamp::now(), delay));
  return runAt(time, cb);
}

TimerId EventLoop::runEvery(double interval, const TimerCallback& cb)
{
  Timestamp time(addTime(Timestamp::now(), interval));
  return timerQueue_->addTimer(cb, time, interval);
}
```

TimerQueue需要高效地组织目前尚未到期的Timer，能快速根据当前时间找到已经到期的Timer，也能高效地添加和删除Timer。
最简单的TimerQueue以按到期时间排好序的线性表为数据结构，常用操作都是线性查找，复杂度是O（N）。
另一种常用做法是二叉堆组织优先队列，这种做法的复杂度是O（NlogN），但是C++标准库中的make_heap不能高效地删除heap中间的某个元素，需要我们自己实现。
还有一种做法是使用二叉搜索树（set或者map），把Timer按到期时间先后排好序，操作复杂度仍然是O（NlogN），但是我们不能直接用map<Timestamp, Timer*>，
因为这样无法处理两个Timer到期时间相同的情况。

采用pair<Timestamp, Timer*>为key，这样即使两个Timer的到期时间相同，它们的地址也必定不同。TimerList是set而非map，因为只有key没有value。
这样也保证了它们插入查找删除的复杂度为O(logN)。
```
typedef std::pair<Timestamp, Timer*> Entry;
typedef std::set<Entry> TimerList;
```
## 保证线程安全函数

在EventLoop里有一个非常重要的功能：在它的IO线程内执行某个用户的任务回调，即EventLoop::runInLoop(const Functor& cb)，
如果用户是在当前IO线程调用这个函数，回调会同步进行；如果用户在其他吸纳从调用runInLoop(), cb会被加入队列，IO线程会被唤醒来调用这个Functor。
```
void EventLoop::runInLoop(const Functor& cb)
{
  if (isInLoopThread())
  {
    cb();
  }
  else
  {
    queueInLoop(cb);
  }
}
```
有了这个功能，我们就能轻易地在线程间调配任务，比方说把TimerQueue的成员函数调用移到其IO线程，这样可以在不用锁的情况下保证线程安全。

由于IO线程平时阻塞在事件循环loop()的poll调用中，为了让IO线程能够立刻执行用户回调，我们需要设法唤醒它。
传统方法是用pipe，IO线程始终监视此管道的readable事件，在需要唤醒的时候，其他线程往管道里写一个字节，这样IO线程就从IO multiplexing阻塞调用中返回。
现在Linux有了eventfd，可以更高效地唤醒，因为它不必管理缓冲区。

queueInLoop将cb放入队列，并在必要时调用wakeup唤醒IO线程。
必要时有两种情况：
* 如果调用的不是IO线程，必须唤醒。
* 如果在IO线程调用queueInLoop(),而此时正在调用pending functor, 也必须唤醒。
只有在IO线程的回调事件回调中调用queueInLoop()才无需wakeup().

```
void EventLoop::queueInLoop(const Functor& cb)
{
  {
    MutexLockGuard lock(mutex_);
    pendingFunctors_.push_back(cb);
  }

  if (!isInLoopThread() || callingPendingFunctors_)
  {
    wakeup();//向wakeupFd_写数据，唤醒它的读事件
  }
}

/*
*doPendingFunctors()把回调列表swap()到局部变量functors中，这样一方面减小了临界区长度
*(不会阻塞其他线程调用queueInLoop()),另一方面也避免了死锁(因为Functor可能会再调用queueInLoop())
*/
void EventLoop::doPendingFunctors()
{
  std::vector<Functor> functors;
  callingPendingFunctors_ = true;

  {
    MutexLockGuard lock(mutex_);
    functors.swap(pendingFunctors_);
  }

  for (size_t i = 0; i < functors.size(); ++i)
  {
    functors[i]();
  }
  callingPendingFunctors_ = false;
}
```




