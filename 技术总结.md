# 技术总结

## 线程同步
线程同步的四项原则，按重要性排列：
1. 首要原则是尽量最低限度地共享对象，减少需要同步的场合。一个对象能不暴露给别的线程就不要暴露；如果要暴露，优先考虑不可修改的对象；实在不行才暴露可修改的对象，并用同步措施来充分保护它。
2. 其次是使用高级的并发编程构件，如TaskQueue、Producer-Consumer Queue、 CountDownLatch等。
3. 最后不得已必须使用底层同步原语时，只用非递归的互斥器和条件变量，慎用读写锁，不要用信号量。
4. 除了使用atomic整数之外，不自己编写lock-free代码，也不要用“内核级”同步原语。

### 互斥锁
使用RAII（资源获取即初始化）技术，对MutexLock初始化和析构进行处理。MutexLockGuard对象初始化的时候加锁，析构的时候解锁。
```
class MutexLockGuard : noncopyable
{
public:
  explicit MutexLockGuard(MutexLock& mutex) : mutex_(mutex)
  {
    mutex_.lock();
  }

  ~MutexLockGuard()
  {
    mutex_.unlock();
  }

private:
  MutexLock& mutex_;
};
```
### 条件变量
1. Blockingqueue

关于封装条件变量的一般使用，假设我们要实现简单的容量无限的BlockingQueue，可以这样写：
```
MutexLock mutex;
Condition cond(mutex);
std::deque<int> queue;

int dequeue()
{
    MutexLockGurad lock(mutex);
    while(queue.empty())
    {
        cond.wait();//这一步会释放mutex并进入等待，这两个操作是原子的
        //wait()返回后，会自动重新加锁
    }
    assert(!queue.empty());
    int top = queue.front();
    queue.pop_front();
    return top;
}

void enqueue(int x )
{
    MutexLockGurad(mutex);
    queue.push_back(x);
    cond.notify();
}
```

2. CountDownLatch

CountDownLatch,本质上来说,是一个thread safe的计数器,用于主线程和工作线程的同步.
它的主要用法有两种:
* 第一种:在初始化时,需要指定主线程需要等待的任务的个数(count),当工作线程完成 Task Callback后对计数器减1，而主线程通过wait()调用阻塞等待技术器减到0为止。**通常用于主线程等待多个子线程完成初始化。**
* 第二种:主线程发起多个子线程，子线程都等待主线程，主线程完成其他任务之后通知所有子线程开始执行。**通常用于多个子线程等待主线程发出起跑命令。**

```
class CountDownLatch : boost::noncopyable
{
 public:

  explicit CountDownLatch(int count)
    : mutex_(),
      condition_(mutex_),
      count_(count)
  {
  }

  void wait()
  {
    MutexLockGuard lock(mutex_);
    while (count_ > 0)
    {
      condition_.wait();
    }
  }

  void countDown()
  {
    MutexLockGuard lock(mutex_);
    --count_;
    if (count_ == 0)
    {
      condition_.notifyAll();
    }
  }

  int getCount() const
  {
    MutexLockGuard lock(mutex_);
    return count_;
  }

 private:
  mutable MutexLock mutex_;
  Condition condition_;
  int count_;
};
```

## 多线程日志
一个多线程程序的每个线程只写一个日志文件，这样分析日志更容易，不必在多个文件中跳来跳去，多线程写多个文件也不一定能提速。
采用“异步日志”的方式，用一个背景线程负责收集日志消息，并写入日志文件, 其他业务线程只管往这个“日志线程”发送日志消息。

**日志库采用的是双缓冲技术，基本思路是准备两块buffer：A和B。前端负责往buffer A填数据，后端负责将buffer B的数据写人文件。
当buffer A写满之后，交换A和B，让后端将buffer A的数据写入文件，而前端则往buffer B填人新的日志消息，如此往复。**

用两个buffer的好处是：在新建日志消息的时候不必等待磁盘文件操作，也避免每条新日志消息都触发后端日志线程。
换言之，前端不是将一条条日志消息分别传送给后端，而是将多条日志消息拼成一个大的buffer传送给后端，相当于批处理，减少了线程唤醒的频度，降低开销。
另外，为了及时将日志消息写人文件，即便buffer A未满，日志库也会每3秒执行一次上述交换写人操作。

**日志大体可以分为前端和后端两部分。前端是供应用程序使用的接口API，并生成日志消息；后端负责把日志消息写到目的地。**

关键代码 base/AsyncLogging.cpp
Buffer 大小为4MB，可以存至少1000条日志消息。mutex_用于保护后面的四个数据成员。buffers_放的是供后端写入的buffer。
```
MutexLock mutex_;
Condition cond_;
BufferPtr currentBuffer_;
BufferPtr nextBuffer_;
BufferVector buffers_;

void append(const char* logline, int len) {
    muduo::MutexLockGuard lock(mutex_);
    if (currentBuffer_->avail() > len)
    {
        currentBuffer_->append(logline, len);
    }
    else
    {
        buffers_.push_back(currentBuffer_.release());
        if (nextBuffer_)
        {
            currentBuffer_ = boost::ptr_container::move(nextBuffer_);
        }
        else
        {
            currentBuffer_.reset(new Buffer); // Rarely happens
        }
        currentBuffer_->append(logline, len);
        cond_.notify();
    }
}

void AsyncLogging::threadFunc() {//后端接收日志
  assert(running_ == true);
  latch_.countDown();
  LogFile output(basename_);
  BufferPtr newBuffer1(new Buffer);//准备两块空闲的日志
  BufferPtr newBuffer2(new Buffer);
  newBuffer1->bzero();
  newBuffer2->bzero();
  BufferVector buffersToWrite;
  buffersToWrite.reserve(16);
  while (running_) {
    assert(newBuffer1 && newBuffer1->length() == 0);
    assert(newBuffer2 && newBuffer2->length() == 0);
    assert(buffersToWrite.empty());

    {
      MutexLockGuard lock(mutex_);
      if (buffers_.empty())  // unusual usage!
      {
        cond_.waitForSeconds(flushInterval_);//触发条件：1，超时 2，前端写满了一个或者多个buffer
      }
      buffers_.push_back(currentBuffer_);
      currentBuffer_.reset();

      currentBuffer_ = std::move(newBuffer1);//将空闲的newBuffer1移为当前缓冲
      buffersToWrite.swap(buffers_);//内部指针交换而非复制，这样代码可以在临界区之外安全的访问buffersToWrite
      if (!nextBuffer_) {
        nextBuffer_ = std::move(newBuffer2);//neWBuffer2替换nextBuffer_，这样前端始终有一个预备buffer可供调配
      }
    }

    assert(!buffersToWrite.empty());

    if (buffersToWrite.size() > 25) {//处理过多的日志堆积问题，直接丢弃
      char buf[256];
      snprintf(buf, sizeof buf, "Dropped log messages at %s, %zd larger buffers\n",
                Timestamp::now().toFormattedString().c_str(),
                buffersToWrite.size()-2);
      fputs(buf, stderr);
      output.append(buf, static_cast<int>(strlen(buf)));
      buffersToWrite.erase(buffersToWrite.begin() + 2, buffersToWrite.end());
    }

    for (size_t i = 0; i < buffersToWrite.size(); ++i) {//将buffersToWrite写到文件中
      output.append(buffersToWrite[i]->data(), buffersToWrite[i]->length());
    }

    if (buffersToWrite.size() > 2) {
      // drop non-bzero-ed buffers, avoid trashing
      buffersToWrite.resize(2);
    }

    if (!newBuffer1) {
      assert(!buffersToWrite.empty());
      newBuffer1 = buffersToWrite.back();
      buffersToWrite.pop_back();
      newBuffer1->reset();
    }

    if (!newBuffer2) {
      assert(!buffersToWrite.empty());
      newBuffer2 = buffersToWrite.back();
      buffersToWrite.pop_back();
      newBuffer2->reset();
    }

    buffersToWrite.clear();
    output.flush();
  }
  output.flush();
}
```

前端在生成一条日志消息的时候会调用AsyncLogging::append();.
1. 如果当前缓冲currentBuffer_剩余的空间足够大，则会直接把日志消息追加到当前缓冲中，这是最常见的情况。这里拷贝一条日志消息并不会带来多大开销。前后端代码的其余部分都没有拷贝，而是简单的指针交换；
2. 如果当前缓冲已经写满，就把它移入buffers_，并试图把预备好的另一块缓冲nextBuffer_移用为当前缓冲；然后追加日志消息并唤醒后端开始写人日志数据。
以上两种情况在临界区之内都没有耗时的操作，运行时间为常数；
3. 如果前端写人速度太快，一下子把两块缓冲都用完了，那么只好分配一块新的buffer作为当前缓冲，这是极少发生的情况。

在接收方（后端）实现AsyncLogging::threadFunc中:
1. 首先准备好两块空闲的buffer，以备在临界区内交换；在临界区内，等待条件触发，这里的条件有两个：其一是超时，其二是前端写满了一个或多个buffer。注意这里是非常规的condition variable用法，它没有使用while循环，而且等待时间有上限。
2. 当“条件”满足时，先将当前缓冲currentBuffer_移人buffers_，并立刻将空闲的newBuffer1移为当前缓冲。这整段代码位于临界区之内，因此不会有任何race condition。
3. 接下来将交换buffers_和buffersToWrite，后面的代码可以在临界区之外安全地访问bufferSToWrite，将其中的数据写入文件。
4. 临界区里最后干的一件事情是用neWBuffer2替换nextBuffer_，这样前端始终有一个预备buffer可供调配。nextBuffer_可以减少前端临界区分配内存的概率，缩短前端临界区长度。注意到后端临界区内也没有耗时的操作，运行时间为常数。
5. 接下来会将 buffersToWrite 内的 buffer 重新填充 newBufferl 和 newBuffer2，这样下一次执行的时候还有两个空闲buffer可用于替换前端的当前缓冲和预备缓冲。

## 网络核心库
本项目是基于Reactor模式的网络库，其核心是个事件循环EventLoop，用于相应计时器和IO事件

主要部件：
1. Buffer：数据的读写通过buffer进行。用户代码不需要调用read()/write()，只需要处理收到的数据和准备好要发送的数据
2. InetAddress：简单封装IPv4地址
3. Socket：封装Socket描述符
3. EventLoop：事件循环（反应器Reactor），每个线程只能有一个 EventLoop实体，它负责IO和定时器事件的分派。它用eventfd()来异步唤醒，这有别于传统的用一对pipe()的办法。
它用TimerQueue作为计时器管理，用Poller作为IO multiplexing的实现
4. EventLoopThread：启动一个线程，在其中运行EventLoop::loop()
5. Channel：用于socket连接事件的分发
6. TcpConnection：整个网络库的核心，封装一次TCP连接，注意它不能发起连接
7. TcpClient：用于编写网络客户端，能发起连接，并且有重试功能
8. TcpServer：用于编写网络服务器，接受客户的连接

## 并发模型

本网络库是并发非阻塞TCP网络编程，它的核心是每个IO线程是一个事件循环，把IO事件分发到回调函数上。
编写网络库的目的之一就是简化日常的TCP网络编程，让程序员能把精力集中在业务逻辑的实现上，而不要天天和Sockets API较劲。

基于事件的非阻塞网络编程是编写高性能并发网络服务程序的主流模式：
头一次使用这种方式编程通常需要转换思维模式：把原来“主动调用recv()来接收数据，主动调用accept()来接受新连接，主动调用send()来发送数据”的思路换成
“注册一个收数据的回调，网络库收到数据会调用我，直接把数据提供给我，供我消费。注册一个接受连接的回调，网络库接受了新连接会回调我，直接把新的连接对象传给我，供我使用。
需要发送数据的时候，只管往连接中写，网络库会负责无阻塞地发送。”

本项目使用的并发模型如下：

![并发模型](https://github.com/coffee0218/WebServer/blob/master/data/model.png)

每个线程最多有一个EventLoop，每个TcpConnection必须归某个EventLoop管理。MainReactor只有一个，负责响应client的连接请求，并建立连接，它使用一个NIO Selector。
Acceptor与MainEventLoop在同一个线程，另外创建一个EventLoopThreadPool，新的连接会按round-robin方式分配到线程中，线程池的大小是固定的，池子的大小通常根据CPU数目确定。

新的TcpConnection连接挂在某个subReactor中，该连接的所有操作都在那个subReactor所处的线程中完成。多个连接可能被分派到多个线程中，以充分利用CPU。
每个subReactor都会在一个独立线程中运行，属于某个EventLoop，并且维护一个独立的NIO Selector。

一个连接完全由MainReactor一个线程管理，请求的顺序性得到保证，同时IO的请求压力比较大时，subReactor也可以分担。
当主线程把新连接分配给了某个SubReactor，该线程此时可能正阻塞在多路选择器(epoll)的等待中，怎么得知新连接的到来呢？这里使用了eventfd进行异步唤醒，线程会从epoll_wait中醒来，得到活跃事件，进行处理。

## Buffer设计
Non-blocking IO 的核心思想是避免阻塞在 read() 或 write() 或其他 IO 系统调用上，这样可以最大限度地复用 thread-of-control，让一个线程能服务于多个 socket 连接。
IO 线程只能阻塞在 IO-multiplexing 函数上，如 select()/poll()/epoll_wait()。这样一来，应用层的缓冲是必须的，每个 TCP socket 都要有input buffer 和 output buffer。
项目对I/O做了一层封装，让网络库与真正的套接字打交道，在应用层只需要利用封装的Buffer完成数据的收发。

Buffer 其实像是一个 queue，从末尾写入数据，从头部读出数据。
•	对外表现为一块连续的内存(char* p, int len)，以方便客户代码的编写。 
•	其 size() 可以自动增长，以适应不同大小的消息。它不是一个 fixed size array (即 char buf[8192])。 
•	内部以 vector of char 来保存数据，并提供相应的访问函数。 

Buffer 的数据结构如下：

![并发模型](https://github.com/coffee0218/WebServer/blob/master/data/buffer.png)

两个 indices 把 vector 的内容分为三块：prependable、readable、writable，各块的大小是（公式一）：
* prependable = readIndex
* readable = writeIndex - readIndex
* writable = size() - writeIndex

readIndex 和 writeIndex 满足以下不变式(invariant): 0 ≤ readIndex ≤ writeIndex ≤ data.size()


